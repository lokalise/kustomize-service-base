apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: rollout
  annotations:
    notifications.argoproj.io/subscribe.on-rollout-updated.slack: k8s-app-rollouts-all
    notifications.argoproj.io/subscribe.on-rollout-progressing.slack: k8s-app-rollouts-all
    notifications.argoproj.io/subscribe.on-rollout-healthy.slack: k8s-app-rollouts-all
    notifications.argoproj.io/subscribe.on-rollout-degraded.slack: k8s-app-rollouts-all
    notifications.argoproj.io/subscribe.on-new-version-released.grafana: service
    notifications.argoproj.io/subscribe.on-new-version-released-shared.grafana: k8s-app-rollouts


spec:
  # Controlled by HPA; do NOT set explicitly! (race condition with HPA)
  # replicas: 1
  strategy:
    canary:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels: {}
  template:
    metadata:
      labels: {}
    spec:
      securityContext:
        runAsUser: 1000
      initContainers: []
      containers:
        # The primary container for our service
        - name: app
          image: base
          imagePullPolicy: Always
          securityContext:
            privileged: false
            allowPrivilegeEscalation: false
            runAsNonRoot: true
          env:
            # Default env variables we want all containers to have
            - name: "POD_NAME"
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: "POD_NAMESPACE"
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: "NAMESPACE"
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - name: app
              containerPort: 8080
              protocol: TCP
          # Pull env variables from other configmaps or secrets
          envFrom:
            - secretRef:
                name: env
          # Liveness probes, which dictate if a pod is healthy or should be replaced
          livenessProbe:
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
            httpGet:
              port: app
              path: /health
          # Readiness probes, which dictate if a pod is ready to receive traffic
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
            httpGet:
              port: app
              path: /health
          # This defines our resource limits
          resources:
            requests:
              memory: "64Mi"
              cpu: "250m"
            limits:
              memory: "128Mi"
              cpu: "500m"
      # Modify /etc/resolv.conf ndots to reduce excess DNS queries
      # foo.bar.com will be resolved; foo.bar, foo go to the search list first
      dnsConfig:
        options:
          - name: ndots
            value: "3"
      # This is pod affinity and anti-affinity, to determine where to schedule pods
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: role
                  operator: In
                  values:
                    - application
                - key: kubernetes.io/arch
                  operator: In
                  values:
                    - arm64
        podAntiAffinity:
          #requiredDuringSchedulingIgnoredDuringExecution:
          #  - topologyKey: "kubernetes.io/hostname"
          #    labelSelector:
          #      matchLabels: {}
          #  - topologyKey: "failure-domain.beta.kubernetes.io/zone"
          #    labelSelector:
          #      matchLabels: {}
          preferredDuringSchedulingIgnoredDuringExecution:
            # We want to prefer (but not require) that pods do not get
            # scheduled on the same node
            - weight: 100
              podAffinityTerm:
                topologyKey: "kubernetes.io/hostname"
                labelSelector:
                  matchLabels: {}
            # We want to prefer (but not require) that pods do not get
            # scheduled on the same availability zone, to get actual redundancy between zone failures
            - weight: 100
              podAffinityTerm:
                topologyKey: "failure-domain.beta.kubernetes.io/zone"
                labelSelector:
                  matchLabels: {}
      # Avoid pods to be deployed to the same node or availability zone
      topologySpreadConstraints:
        - labelSelector:
            matchLabels: {}
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: DoNotSchedule
        - labelSelector:
            matchLabels: {}
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
